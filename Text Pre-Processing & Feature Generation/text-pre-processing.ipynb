{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing & Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries for all tasks\n",
    "import requests # It also allows you to access the response data of Python.Download pdf using respective links\n",
    "import re # for working with Regular Expressions.\n",
    "import pandas as pd # for data manipulation and analysis\n",
    "import nltk.data # The Natural Language Toolkit used for text processing libraries for tokenization, parsing, etc\n",
    "import itertools # used chain method from intertools to merge sublists\n",
    "from nltk.tokenize import RegexpTokenizer # tokenizer uses regex to tokenize\n",
    "from nltk.probability import FreqDist # used to generate frequency distribution\n",
    "from nltk.stem import PorterStemmer # used to stem tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  <font size= '3'> **re.sub(x,y,z)**   :   Replaces y with x in z.Operation is done on strings and return value is a string \n",
    "* **re.findall(x,y)**   :   Finds x pattern/string in y and returns the matches in a list\n",
    "* **len()**    :    The len() function returns the number of items in an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract text from pdf\n",
    "import io # for stream handling\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter #libraries for extracting info from PDF, analysing data etc\n",
    "from pdfminer.converter import TextConverter #converting PDF files into other text formats\n",
    "from pdfminer.layout import LAParams #performing layout analysis\n",
    "from pdfminer.pdfpage import PDFPage # processing the page contents\n",
    "\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                                  password=password,\n",
    "                                  caching=caching,\n",
    "                                  check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting links from pdf\n",
    "links_text  = convert_pdf_to_txt('paper-ids.pdf') # calling convert_pdf_to_txt function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern to extarct pdf id and respective pdf download link\n",
    "pattern = re.compile(r'(PP[\\d]+.pdf) (https:[\\w\\W]+?)\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PP3197.pdf',\n",
       " 'https://drive.google.com/uc?export=download&id=1z9rts_mqD0rk1cztTomCZEmYu2Xv10iv')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_list = re.findall(pattern,links_text) #extarct pdf id and respective pdf download link\n",
    "links_list[0] #item 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading PDF files from links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#5-6 min runtime for this code\n",
    "\n",
    "for each in links_list:\n",
    "    filename,link = each\n",
    "    r = requests.get(link) # pass each link to requests() method\n",
    "    with open(filename,'wb') as code:\n",
    "        code.write(r.content) # write the content into PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# using convert_pdf_to_txt function to convert all 200 PDF to text. Text is stored in a list of lists\n",
    "\n",
    "all_doc_list = []\n",
    "for filename,link in links_list:\n",
    "    each_doc = convert_pdf_to_txt(filename)\n",
    "    all_doc_list.append(each_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting title,abstract,body and authors from all the 200 document sublists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  re.compile for compiling pattern into pattern objects for body, title, abstract and authors.\n",
    "body_pattern = re.compile(r'Paper Body([\\W\\w]+)[\\d]+ References')\n",
    "title_pattern = re.compile(r'([\\W\\w\\s\\S]+?)Authored by:(.*)')\n",
    "abstract_pattern = re.compile(r'Abstract([\\w\\W\\s\\S]+?)Paper Body')\n",
    "authors_pattern = re.compile(r'Authored by:\\n\\n([\\w\\W]+?)\\n\\nAbstract')\n",
    "# creating empty lists for body, title, abstract and authors.\n",
    "all_body_list = []\n",
    "all_title = []\n",
    "all_abstract = []\n",
    "all_authors = []\n",
    "# extracting body, title, abstract and authors and appending them into a list.\n",
    "for each in all_doc_list:\n",
    "    all_body_list.append(re.search(body_pattern,each).group(1))\n",
    "    all_title.append(re.search(title_pattern,each).group(1))\n",
    "    all_abstract.append(re.search(abstract_pattern,each).group(1))\n",
    "    all_authors.append(re.search(authors_pattern,each).group(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do the sentence segmentation of the body, we need to clean the body. The below function is defined to clean the body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the body\n",
    "def clean_body(body):\n",
    "    re_list = ['\\n','- ','\\?\\d\\s\\?\\s','\\?\\d\\s','-\\?\\s','-?','\\?','ﬀ','ﬁ','ﬃ','ﬄ','ﬂ'] # to replace char\n",
    "    sub_list = [' ','','','','','','\\'','ff','fi','ffi','ffl','fl']  # by replace char\n",
    "    for i in range(0,len(re_list)):\n",
    "        body = re.sub(re_list[i],sub_list[i],body).strip()\n",
    "    return body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Sentence Segmentation of body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence segmentation\n",
    "all_body_sentence = [] # creating an empty list for all body.\n",
    "for each_body in all_body_list:\n",
    "    body = clean_body(each_body)\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle') # this tokenizer divides a text into a list of sentences\n",
    "    sentences = sent_detector.tokenize(body) # for converting text strings to streams of token objects\n",
    "    all_body_sentence.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Current reinforcementlearning (RL) techniques hold great promise for creating a general type of artificial intelligence (AI), specifically autonomous (software) agents that learn difficult tasks with limited feedback (Sutton & Barto, 1998).',\n",
       " 'Applied RL has been very successful, producing worldclass computer backgammon players (Tesauro, 1994) and model helicopter flyers (Ng et al., 2003).',\n",
       " 'Many applications of RL, including the two above, utilize supervisedlearning techniques for the purpose of generalization.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_body_sentence[0][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting first capital letters to lower case for each sentence** <br>\n",
    "**normalizing to lowercase except the capital tokens appearing in the\n",
    "middle of a sentence/line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first capital letter to lower()\n",
    "total_body_sentences = [] # stores all sentences as list of lists\n",
    "for each in all_body_sentence:\n",
    "    each_sentence_lower = [] # stores sentences for each body\n",
    "    for sentence in each:\n",
    "        sentence = sentence[0].lower()+sentence[1:] # slicing the sentence to lower the first char\n",
    "        each_sentence_lower.append(sentence)\n",
    "    total_body_sentences.append(''.join(each_sentence_lower)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_body_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Tokenization <br>\n",
    "**Word tokenization of body using regular expression, r\"[A-Za-z]\\w+(?:[- '?]\\w+)?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import RegexpTokenizer\n",
    "unigram_tokens= []\n",
    "for each in total_body_sentences:\n",
    "    tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\") # regex to tokenize\n",
    "    unigram_tokens.append(tokenizer.tokenize(each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reinforcementlearning', 'RL', 'techniques', 'hold']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokens[0][1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G. First 200 meaningful bigrams, based on highest total frequency in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating bigrams \n",
    "all_doc_bigrams = []\n",
    "for each_doc_token_list in unigram_tokens:\n",
    "    per_doc_bigrams = list(nltk.bigrams(each_doc_token_list)) # nltk.bigrams() to generate bigrams from an input list\n",
    "    all_doc_bigrams.append(per_doc_bigrams)\n",
    "# all_doc_bigrams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reinforcementlearning', 'RL'),\n",
       " ('RL', 'techniques'),\n",
       " ('techniques', 'hold'),\n",
       " ('hold', 'great')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_bigrams[0][1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading stopwords.txt and storing stopwords in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', \"a's\", 'able', 'about', 'above']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the given text file to get context_independent_tokens \n",
    "with open('stopwords_en.txt') as stop:\n",
    "    context_independent_tokens = stop.read().split('\\n')\n",
    "context_independent_tokens[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all the bigrams with Stop words as part of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# each_do_bigrams = []\n",
    "all_doc_bigrams_no_stop = [] # stores all bigrams as list of lists\n",
    "for each_doc_bigrams in all_doc_bigrams:\n",
    "    each_do_bigrams = [] # list stores each document bigrams\n",
    "    for each_bigram in each_doc_bigrams:\n",
    "        x,y = each_bigram # tuple unpacking (into two unigrams)\n",
    "        if x.lower() in context_independent_tokens or y.lower() in context_independent_tokens:\n",
    "            pass\n",
    "        else:\n",
    "            bigram_new = x+'__'+y # joining the tokens again with '__' double underscore\n",
    "            each_do_bigrams.append(bigram_new)\n",
    "    all_doc_bigrams_no_stop.append(each_do_bigrams)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reinforcementlearning__RL',\n",
       " 'RL__techniques',\n",
       " 'techniques__hold',\n",
       " 'hold__great']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_bigrams_no_stop[0][1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Freq distribution of bigrams across all 200 docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Linear__Regression': 8, 'yi__xi': 7, 'valid__prediction': 7, 'linear__regression': 6, 'parameter__vector': 6, 'high__probability': 6, 'sample__complexity': 6, 'RL__algorithm': 5, 'state__space': 5, 'ith__row': 5, ...})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.probability import FreqDist\n",
    "all_doc_freq_dist_bigrams = []\n",
    "for each_doc_list in all_doc_bigrams_no_stop:\n",
    "    fdbigram = FreqDist(each_doc_list) # using FreqDist() to calculate distribution of each token in the list\n",
    "    all_doc_freq_dist_bigrams.append(fdbigram)\n",
    "all_doc_freq_dist_bigrams[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using intertools to join all the lists containing freq distributions of bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['current__reinforcementlearning',\n",
       " 'reinforcementlearning__RL',\n",
       " 'RL__techniques',\n",
       " 'techniques__hold',\n",
       " 'hold__great']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import itertools\n",
    "combined_bigrams = list(itertools.chain.from_iterable(all_doc_freq_dist_bigrams)) #combine all sublist's into one\n",
    "combined_bigrams[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machine__learning', 81),\n",
       " ('figure__shows', 71),\n",
       " ('optimization__problem', 71),\n",
       " ('future__work', 66),\n",
       " ('Processing__Systems', 63)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting top 200 frequently occuring bigrams in the corpus\n",
    "top_200_bigrams_dist = FreqDist(combined_bigrams).most_common(200) #using FreqDist() to calculate dist of each token in the list\n",
    "top_200_bigrams_dist[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine__learning',\n",
       " 'figure__shows',\n",
       " 'optimization__problem',\n",
       " 'future__work',\n",
       " 'Processing__Systems']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting top 200 bigram words from the freq distribution list\n",
    "top_200_bigrams = []\n",
    "for i in range(0,len(top_200_bigrams_dist)):\n",
    "    x,y = top_200_bigrams_dist[i]\n",
    "    top_200_bigrams.append(x)   \n",
    "top_200_bigrams[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. The context-independent and context-dependent (with the threshold set to %95) stop words to be removed from the vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing StopWords from unigram tokens across all 200 sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing context independent stopwords from unigram tokens\n",
    "unigram_tokens_no_stop = []\n",
    "for each_unigram_list in unigram_tokens:\n",
    "    # \n",
    "    stopped_tokens = [w for w in each_unigram_list if w.lower() not in context_independent_tokens] # iterating across sublists\n",
    "    unigram_tokens_no_stop.append(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reinforcementlearning', 'RL', 'techniques', 'hold']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokens_no_stop[0][1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting unique tokens from each sublist into a new list of sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['utilize', 'computational', 'normconstrained', 'development']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extarcting unique tokens from unigram_tokens_no_stop for each document\n",
    "unigram_tokens_no_stop_unique = []\n",
    "for each_unigram_list in unigram_tokens_no_stop:\n",
    "    unigram_tokens_no_stop_unique.append(list(set(each_unigram_list)))\n",
    "unigram_tokens_no_stop_unique[0][1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all the unique unigrams from all 200 sublists into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_unigram_tokens = list(itertools.chain.from_iterable(unigram_tokens_no_stop_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156140"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_unigram_tokens) # total unigram tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unigrams that occur in more than 95% of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shown', 169),\n",
       " ('utilize', 21),\n",
       " ('computational', 110),\n",
       " ('normconstrained', 1),\n",
       " ('development', 36)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.95*200 # 200 is the total documents, threshold=190\n",
    "more_than_95_unigm = []\n",
    "less_than_95_unigm = []\n",
    "for each in FreqDist(combined_unigram_tokens).items():\n",
    "    x,y = each\n",
    "    if y>threshold:\n",
    "        more_than_95_unigm.append(each) # list contains unigrams present in more than 95% of documents\n",
    "    else:\n",
    "        less_than_95_unigm.append(each) # list contains unigrams present less than 95% of documents\n",
    "less_than_95_unigm[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting unique bigrams from each sublist into a new list of sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extarcting unique tokens from all_doc_bigrams_no_stop for each document\n",
    "bigrams_tokens_no_stop_unique = []\n",
    "for each_bigram_list in all_doc_bigrams_no_stop:\n",
    "    bigrams_tokens_no_stop_unique.append(list(set(each_bigram_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all the unique bigrams from all 200 sublists into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_bigram_tokens = list(itertools.chain.from_iterable(bigrams_tokens_no_stop_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing bigrams that occur in more than 95% of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machine__learning', 81),\n",
       " ('figure__shows', 71),\n",
       " ('optimization__problem', 71),\n",
       " ('future__work', 66),\n",
       " ('Processing__Systems', 63)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing bigrams that occur in more than 95% of the documents\n",
    "threshold = 0.95*200 # 200 is the total documents, threshold=190\n",
    "more_than_95_bigm = []\n",
    "less_than_95_bigm = []\n",
    "for each in sorted(FreqDist(combined_bigram_tokens).items(),key=lambda x:x[1],reverse=True):\n",
    "    x,y = each\n",
    "    if y>threshold:\n",
    "        more_than_95_bigm.append(each) # list contains bigrams present in more than 95% of documents\n",
    "    else:\n",
    "        less_than_95_bigm.append(each)   # list contains bigrams present in less than 95% of documents\n",
    "less_than_95_bigm[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Rare tokens (with the threshold set to 3%) must be removed from the vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shown', 169),\n",
       " ('utilize', 21),\n",
       " ('computational', 110),\n",
       " ('development', 36),\n",
       " ('Markov', 53)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing unigrams that occur in less than 3% of the documents\n",
    "threshold = 0.03*200 # 200 is the total documents, threshold=6\n",
    "less_than_3_unigm = []\n",
    "more_than_3_unigm = []\n",
    "for each in less_than_95_unigm:\n",
    "    x,y = each\n",
    "    if y<threshold:\n",
    "        less_than_3_unigm.append(each) # list contains unigrams present in less than 3% of documents\n",
    "    else:\n",
    "        more_than_3_unigm.append(each) # list contains unigrams present in more than 3% of documents\n",
    "more_than_3_unigm[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machine__learning', 81),\n",
       " ('figure__shows', 71),\n",
       " ('optimization__problem', 71),\n",
       " ('future__work', 66),\n",
       " ('Processing__Systems', 63)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing bigrams that occur in less than 3% of the documents\n",
    "threshold = 0.03*200 # 200 is the total documents, threshold=6\n",
    "less_than_3_bigm = []\n",
    "more_than_3_bigm = []\n",
    "for each in less_than_95_bigm:\n",
    "    x,y = each\n",
    "    if y<threshold:\n",
    "        less_than_3_bigm.append(each) # list contains bigrams present in less than 3% of documents\n",
    "    else:\n",
    "        more_than_3_bigm.append(each)  # list contains bigrams present in more than 3% of documents\n",
    "more_than_3_bigm[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all the unigrams and bigrams after satisfying the above conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shown', 'utilize', 'computational', 'development', 'Markov']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating final vocabulary with unigrams\n",
    "first_vocab_unigrams = []\n",
    "for each in more_than_3_unigm:\n",
    "    x,y=each\n",
    "    first_vocab_unigrams.append(x)\n",
    "first_vocab_unigrams[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine__learning',\n",
       " 'figure__shows',\n",
       " 'optimization__problem',\n",
       " 'future__work',\n",
       " 'Processing__Systems']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_vocab_bigrams = []\n",
    "for each in more_than_3_bigm:\n",
    "    bigram,freq = each\n",
    "    if bigram in top_200_bigrams:\n",
    "        first_vocab_bigrams.append(bigram)\n",
    "    else:\n",
    "        pass\n",
    "first_vocab_bigrams[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the lists into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shown', 'utilize', 'computational', 'development', 'Markov']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_initial = []\n",
    "vocab_initial.append(first_vocab_unigrams)\n",
    "vocab_initial.append(first_vocab_bigrams)\n",
    "first_vocab_combined = list(itertools.chain.from_iterable(vocab_initial))\n",
    "first_vocab_combined[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4734"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_vocab_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F. Tokens with the length less than 3 should be removed from the vocab.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shown', 'utilize', 'computational', 'development', 'Markov']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_vocab = []\n",
    "for each_token in first_vocab_combined:\n",
    "    if len(each_token)<3: # checking tokens with length less than 3\n",
    "        pass\n",
    "    else:\n",
    "        second_vocab.append(each_token) # tokens with len>=3\n",
    "second_vocab[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4408"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(second_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### C. Unigram stemming using the Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Barcelona__Spain',\n",
       " 'Beach__CA',\n",
       " 'CA__USA',\n",
       " 'Figure__Comparison',\n",
       " 'Figure__Left',\n",
       " 'Figure__shows']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "pattern = re.compile('[\\w]+__[\\w]+') # this pattern will match bigrams from input list\n",
    "vocab = []\n",
    "for each in second_vocab:\n",
    "    if re.search(pattern,each):\n",
    "        vocab.append(each)\n",
    "    else:\n",
    "        vocab.append(stemmer.stem(each)) # stem unigrams from \n",
    "vocab = list(set(vocab)) # taking unique values into a list\n",
    "vocab = sorted(vocab,key=str) # sorting the vocab words\n",
    "vocab[0:6] # display top 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2343"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab) # total words in vocab (unigrams + bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Vocabulary index file <br>\n",
    "**Format:** token_string:token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt','w+',encoding='utf-8') as textfile:\n",
    "    for i in range(0,len(vocab)):\n",
    "        textfile.write('{}:{}'.format(vocab[i],i))\n",
    "        textfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Sparse count vectors file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generating Sparse count vector we require all the unigrams and bigrams from a particular document to be together.So the below code creates a list of lists to store all uni and bi grams for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['current__reinforcementlearning', 'reinforcementlearning__RL', 'RL__techniques', 'techniques__hold', 'hold__great']\n",
      "['current', 'reinforcementlearning', 'RL', 'techniques', 'hold']\n"
     ]
    }
   ],
   "source": [
    "print(all_doc_bigrams_no_stop[0][0:5])\n",
    "print(unigram_tokens_no_stop[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'algorithm': 53, 'state': 36, 'xt': 22, 'input': 19, 'RL': 17, 'prediction': 17, 'number': 16, 'linear': 16, 'problem': 16, 'MDP': 16, ...})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a combined list of unigrams and bigrams for all 200 documents\n",
    "# import itertools\n",
    "uni_bi_zipped_list = []\n",
    "uni_bi_grams_list = [] # to store all uni and bi grams for all 200 docs as sublists\n",
    "for i in range(200):\n",
    "    uni_bi_zipped_list.append(unigram_tokens_no_stop[i]) # extarcting unigrams from each sublist\n",
    "    uni_bi_zipped_list.append(all_doc_bigrams_no_stop[i]) # extarcting bigrams from each sublist\n",
    "    tokens_combined = list(itertools.chain.from_iterable(uni_bi_zipped_list)) # combining uni and bi grams for each sublist(doc)\n",
    "    uni_bi_zipped_list = []\n",
    "    uni_bi_grams_list.append(tokens_combined)\n",
    "    \n",
    "# generating freq distribution for each sublist of uni_bi_grams_list     \n",
    "uni_bi_grams_list_freq = []\n",
    "for i in range(len(uni_bi_grams_list)):\n",
    "    freq = FreqDist(uni_bi_grams_list[i])\n",
    "    uni_bi_grams_list_freq.append(freq)\n",
    "uni_bi_grams_list_freq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting 200 document id's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PP3197', 'PP3234', 'PP3244', 'PP3252', 'PP3389']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_doc_ids = []\n",
    "for each in links_list:\n",
    "    doc_id,link = each\n",
    "    pdf_doc_ids.append(doc_id[0:6])  \n",
    "pdf_doc_ids[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing into sparse count vectors textfile: <br>\n",
    "**Format :** paper_id, token1_index:token1_wordcount, token2_index:token2_wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open('count_vectors.txt','w+') as count_vector_file:\n",
    "    for i in range(200): # i used to iterate through paper_id and uni_bi_grams_list_freq\n",
    "        paper_id = pdf_doc_ids[i]\n",
    "        count_vector_file.write(paper_id) # writing paper_id to the document\n",
    "        for each_token in vocab: # comparing each token vocab with tokens from each document\n",
    "            if each_token in list(uni_bi_grams_list_freq[i].keys()): # checking in tokens of each document\n",
    "                freq_each_token = uni_bi_grams_list_freq[i][each_token]\n",
    "                index_of_each_token = vocab.index(each_token)\n",
    "                count_vector_file.write(',{}:{}'.format(index_of_each_token,freq_each_token)) # token1_index:token1_wordcount\n",
    "        count_vector_file.write('\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Abstracts, Tokens must be normalized to lowercase except the capital tokens appearing in the middle of a sentence/line. (use sentence segmentation to achieve this). For Titles, tokens must be all normalised to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWe provide a provably eﬃcient algorithm for learning Markov Decision\\nProcesses (MDPs) with continuous state and action spaces in the online\\nsetting. Speciﬁcally, we take a model-based approach and show that a\\nspecial type of online linear regression allows us to learn MDPs with (pos-\\nsibly kernalized) linearly parameterized dynamics. This result builds on\\nKearns and Singh’s work that provides a provably eﬃcient algorithm for\\nﬁnite state MDPs. Our approach is not restricted to the linear setting,\\nand is applicable to other classes of continuous MDPs.\\n\\n1 '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_abstract[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we clean the abstarcts by iterating through the entitre list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstarct cleaning\n",
    "all_clean_abstract=[]\n",
    "for each in all_abstract:\n",
    "    each = re.sub('\\n1','',each) # substituting \n",
    "    each = re.sub('\\n\\n','',each)\n",
    "    each = re.sub('\\n',' ',each).strip()\n",
    "    all_clean_abstract.append(each)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We provide a provably eﬃcient algorithm for learning Markov Decision Processes (MDPs) with continuous state and action spaces in the online setting. Speciﬁcally, we take a model-based approach and show that a special type of online linear regression allows us to learn MDPs with (pos- sibly kernalized) linearly parameterized dynamics. This result builds on Kearns and Singh’s work that provides a provably eﬃcient algorithm for ﬁnite state MDPs. Our approach is not restricted to the linear setting, and is applicable to other classes of continuous MDPs.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_clean_abstract[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We provide a provably eﬃcient algorithm for learning Markov Decision Processes (MDPs) with continuous state and action spaces in the online setting.',\n",
       " 'Speciﬁcally, we take a model-based approach and show that a special type of online linear regression allows us to learn MDPs with (pos- sibly kernalized) linearly parameterized dynamics.',\n",
       " 'This result builds on Kearns and Singh’s work that provides a provably eﬃcient algorithm for ﬁnite state MDPs.',\n",
       " 'Our approach is not restricted to the linear setting, and is applicable to other classes of continuous MDPs.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence segmentation\n",
    "all_abstract_sentence = []\n",
    "for each_abstract in all_clean_abstract:\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle') # this tokenizer divides a text into a list of sentences\n",
    "    sentences = sent_detector.tokenize(each_abstract) # for converting text strings to streams of token objects\n",
    "    all_abstract_sentence.append(sentences)\n",
    "all_abstract_sentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting first capital letters to lower case for each sentence** <br>\n",
    "**normalizing to lowercase except the capital tokens appearing in the\n",
    "middle of a sentence/line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first capital letter to lower()\n",
    "total_abstract_sentences = []\n",
    "for each in all_abstract_sentence: # interating through sublists\n",
    "    each_sentence_lower = []\n",
    "    for sentence in each: # interating through sentences in each sublist\n",
    "        sentence = sentence[0].lower()+sentence[1:] # using string slicing to lower case the first char\n",
    "        each_sentence_lower.append(sentence)\n",
    "    total_abstract_sentences.append(''.join(each_sentence_lower)) # appending sentences to a list as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we provide a provably eﬃcient algorithm for learning Markov Decision Processes (MDPs) with continuous state and action spaces in the online setting.speciﬁcally, we take a model-based approach and show that a special type of online linear regression allows us to learn MDPs with (pos- sibly kernalized) linearly parameterized dynamics.this result builds on Kearns and Singh’s work that provides a provably eﬃcient algorithm for ﬁnite state MDPs.our approach is not restricted to the linear setting, and is applicable to other classes of continuous MDPs.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_abstract_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Online Linear Regression and Its Application to\\n\\nModel-Based Reinforcement Learning\\n\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_title[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we clean the titles by iterating through the entitre list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clean_title=[]\n",
    "for each in all_title:\n",
    "    each = re.sub('\\n\\n',' ',each)\n",
    "    each = re.sub('\\n','',each).strip()\n",
    "    all_clean_title.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Online Linear Regression and Its Application to Model-Based Reinforcement Learning'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_clean_title[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean the titles by iterating through the entitre list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michael L. Littman\\nAlexander L. Strehl'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_authors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael L. Littman', 'Alexander L. Strehl']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_authors_clean = []\n",
    "for each in all_authors:\n",
    "    each = re.sub('\\n\\n','\\n',each).split('\\n')\n",
    "    all_authors_clean.append(each)\n",
    "all_authors_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing abstarcts in each 200 sublist's** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['guided', 'by', 'the', 'goal', 'of']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.tokenize import RegexpTokenizer\n",
    "abstract_tokens= []\n",
    "for each in total_abstract_sentences:\n",
    "    tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "    abstract_tokens.append(tokenizer.tokenize(each))\n",
    "abstract_tokens[1][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing title's in each 200 sublist's** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topmoumoute', 'online', 'natural', 'gradient', 'algorithm']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.tokenize import RegexpTokenizer\n",
    "title_tokens= []\n",
    "for each in all_clean_title:\n",
    "    tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "    title_tokens.append(tokenizer.tokenize(each.lower()))\n",
    "title_tokens[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing context independent stop words from abstarct and title**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_tokens_no_stop = []\n",
    "for each_abstract_list in abstract_tokens:\n",
    "    stopped_tokens = [w for w in each_abstract_list if w.lower() not in context_independent_tokens]\n",
    "    abstract_tokens_no_stop.append(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['provide', 'provably', 'eﬃcient', 'algorithm', 'learning']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_tokens_no_stop[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['online', 'linear', 'regression', 'application', 'model-based']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tokens_no_stop = []\n",
    "for each_title_list in title_tokens:\n",
    "    stopped_tokens = [w for w in each_title_list  if w.lower() not in context_independent_tokens]\n",
    "    title_tokens_no_stop.append(stopped_tokens)\n",
    "title_tokens_no_stop[0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we combine all the abstarct tokens, title tokens and authors into three lists respectively**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['provide', 'provably', 'eﬃcient', 'algorithm']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_abstract_tokens = list(itertools.chain.from_iterable(abstract_tokens_no_stop))\n",
    "combined_abstract_tokens[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['online', 'linear', 'regression', 'application']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_title_tokens = list(itertools.chain.from_iterable(title_tokens_no_stop))\n",
    "combined_title_tokens[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael L. Littman', 'Alexander L. Strehl', 'Yoshua Bengio']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_authors = list(itertools.chain.from_iterable(all_authors_clean))\n",
    "combined_authors[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Tasks: <br>\n",
    "* Creating a freq distribution disctionaries for abstarct tokens, title tokens and author's, using **FreqDist()** method\n",
    "* Extracting all the items from the respective dictionaries and converting them into a list, using **.items()**\n",
    "* sort the list using **sorted()** method.\n",
    "* first decreasing sort for frequencies (-x[1]) and then increasing sort for tokens (x[0])\n",
    "* selecting top 10 from the sorted list by slicing through the list ([0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning',\n",
       " 'data',\n",
       " 'model',\n",
       " 'algorithm',\n",
       " 'problem',\n",
       " 'show',\n",
       " 'method',\n",
       " 'based',\n",
       " 'methods',\n",
       " 'problems']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_abstract_tokens_sorted = [x for x,y in sorted(list(FreqDist(combined_abstract_tokens).items()),key=lambda x: (-x[1],x[0]),reverse=False)[0:10]]\n",
    "combined_abstract_tokens_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning',\n",
       " 'models',\n",
       " 'networks',\n",
       " 'deep',\n",
       " 'model',\n",
       " 'data',\n",
       " 'estimation',\n",
       " 'regression',\n",
       " 'sparse',\n",
       " 'visual']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_title_tokens_sorted = [x for x,y in sorted(list(FreqDist(combined_title_tokens).items()),key=lambda x: (-x[1],x[0]),reverse=False)[0:10]]\n",
    "combined_title_tokens_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ambuj Tewari',\n",
       " 'Tong Zhang',\n",
       " 'Trevor Darrell',\n",
       " 'Alex J. Smola',\n",
       " 'Bertrand Thirion',\n",
       " 'Pradeep K. Ravikumar',\n",
       " 'Remi Munos',\n",
       " 'Richard Zemel',\n",
       " 'Tamir Hazan',\n",
       " 'Alessandro Lazaric']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_authors_sorted = [x for x,y in sorted(list(FreqDist(combined_authors).items()),key=lambda x: (-x[1],x[0]),reverse=False)[0:10]]\n",
    "combined_authors_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataframe <br>\n",
    "##### Tasks :\n",
    "* Create an empty data frame with three columns['top10_terms_in_abstracts','top10_terms_in_titles','top10_authors']\n",
    "* Insert the top 10 values from the lists into each column according to the label respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['top10_terms_in_abstracts','top10_terms_in_titles','top10_authors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['top10_terms_in_abstracts']=combined_abstract_tokens_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['top10_terms_in_titles']=combined_title_tokens_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['top10_authors']=combined_authors_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top10_terms_in_abstracts</th>\n",
       "      <th>top10_terms_in_titles</th>\n",
       "      <th>top10_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learning</td>\n",
       "      <td>learning</td>\n",
       "      <td>Ambuj Tewari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>models</td>\n",
       "      <td>Tong Zhang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model</td>\n",
       "      <td>networks</td>\n",
       "      <td>Trevor Darrell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>algorithm</td>\n",
       "      <td>deep</td>\n",
       "      <td>Alex J. Smola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>problem</td>\n",
       "      <td>model</td>\n",
       "      <td>Bertrand Thirion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>show</td>\n",
       "      <td>data</td>\n",
       "      <td>Pradeep K. Ravikumar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>method</td>\n",
       "      <td>estimation</td>\n",
       "      <td>Remi Munos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>based</td>\n",
       "      <td>regression</td>\n",
       "      <td>Richard Zemel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>methods</td>\n",
       "      <td>sparse</td>\n",
       "      <td>Tamir Hazan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>problems</td>\n",
       "      <td>visual</td>\n",
       "      <td>Alessandro Lazaric</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  top10_terms_in_abstracts top10_terms_in_titles         top10_authors\n",
       "0                 learning              learning          Ambuj Tewari\n",
       "1                     data                models            Tong Zhang\n",
       "2                    model              networks        Trevor Darrell\n",
       "3                algorithm                  deep         Alex J. Smola\n",
       "4                  problem                 model      Bertrand Thirion\n",
       "5                     show                  data  Pradeep K. Ravikumar\n",
       "6                   method            estimation            Remi Munos\n",
       "7                    based            regression         Richard Zemel\n",
       "8                  methods                sparse           Tamir Hazan\n",
       "9                 problems                visual    Alessandro Lazaric"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df # data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the dataframe into a csv file using **dataframe.to_csv()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('stats.csv',index=None,header=True)  # generating a CSV file from a data frame using .to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <br>\n",
    "* Pandas 0.25.1 documentation https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html\n",
    "* re - Regular Expression Opeartions https://docs.python.org/2/library/re.html\n",
    "* re - Regular Expression Operations https://docs.python.org/3/library/re.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
